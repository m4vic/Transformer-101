{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxzAewzCDStlE0apgrJzI7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m4vic/Transformer-101/blob/main/Encoder/Decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MiniGPT Decoder.\n",
        "dataset - WikiText-103"
      ],
      "metadata": {
        "id": "blFd3eVklUz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loading → Tokenization → Encode IDs → Padding\n",
        "`→ Token Embedding → Positional Encoding`\n",
        "`→ Masked Multi-Head Self Attention`\n",
        "`→ Add & Norm → Feed Forward → Add & Norm`\n",
        "`→ Repeat N layers`\n",
        "`→ Linear Output Layer → Softmax`\n",
        "`→ Loss (CrossEntropy) → Training Loop`\n",
        "`→ Inference (Text Generation)`\n",
        "`\n"
      ],
      "metadata": {
        "id": "sOQrw6uES2Bo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DataLoading**"
      ],
      "metadata": {
        "id": "5MmhshNPl54M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bX_l9iDxjNYX",
        "outputId": "7875e42c-069e-4a57-bd94-aafe1269f6a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BDy4LweDKLH",
        "outputId": "e484e52a-2986-4acb-f956-03e5674576e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    test: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 4358\n",
            "    })\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 1801350\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3760\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "def build_vocab(texts, vocab_size=50000):\n",
        "  words = []\n",
        "  for text in texts:\n",
        "    text = text.lower()\n",
        "    words.extend(re.findall(r\"\\b\\w+\\b\", text))\n",
        "\n",
        "  #count freq\n",
        "  word_counts = Counter(words)\n",
        "\n",
        "  #keep top vocab size\n",
        "\n",
        "  most_common = word_counts.most_common(vocab_size-2) # reserving 2 for <unk and <pad>\n",
        "\n",
        "  # create word-> id mapping\n",
        "\n",
        "  vocab = {\"<pad>\":0, \"<unk>\":1}\n",
        "  for i, (word,_) in enumerate(most_common,start=2): # it gives us index, word, count\n",
        "    vocab[word] = i  # start from 2 and assign the id to word .\n",
        "  return vocab\n",
        "\n",
        "# build vocab from training split\n",
        "\n",
        "train_texts = [d[\"text\"] for d in dataset[\"train\"]]\n",
        "vocab = build_vocab(train_texts, vocab_size=50000)\n",
        "inv_vocab = {idx: word for word, idx in vocab.items()}\n",
        "print(\"Vocab size:\", len(vocab))\n"
      ],
      "metadata": {
        "id": "hnzF88P8jP3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f7d11e7-0cdf-48f4-bdac-d38c96b5d0e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(text, vocab):\n",
        "  text = text.lower()\n",
        "\n",
        "  words = re.findall(r\"\\b\\w+\\b\", text)\n",
        "  token_ids = [vocab.get(word, vocab[\"<unk>\"]) for word in words] # look for word in words if it exist give the id if not rwturn 0 for unk\n",
        "\n",
        "  return token_ids\n",
        "sample_text = dataset[\"train\"][0][\"text\"][:100]\n",
        "print(\"Sample text:\", sample_text)\n",
        "print(\"Token IDs:\", tokenize_text(sample_text, vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU3fcacRGaa3",
        "outputId": "1c5aded1-2e62-49c7-d820-07c0a0502bb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample text: \n",
            "Token IDs: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "SEQ_LEN = 128\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "def encode_dataset(dataset_split, vocab):\n",
        "  all_ids =[]\n",
        "  for d in dataset_split:\n",
        "    token_ids = tokenize_text(d[\"text\"], vocab)\n",
        "    #split into chunks of seq_len\n",
        "    for i in range(0, len(token_ids), SEQ_LEN): # 0 to len of token ids to step seq len\n",
        "      chunk = token_ids[i:i+SEQ_LEN] # EACH TIME it takes token id of len i+128\n",
        "      if len(chunk) < SEQ_LEN: #\n",
        "        chunk += [vocab[\"<pad>\"]] * (SEQ_LEN - len(chunk))\n",
        "      all_ids.append(chunk)\n",
        "  return all_ids\n",
        "\n",
        "train_encoded = encode_dataset(dataset[\"train\"], vocab)\n",
        "all_sequences = torch.tensor(train_encoded, dtype=torch.long)\n",
        "\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "train_size = int(0.9 * len(all_sequences))\n",
        "val_size   = len(all_sequences) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(all_sequences, [train_size, val_size])\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "metadata": {
        "id": "ty3tNoULjTrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TOKEN EMBEDDING + POSITIONAL ENCODING**"
      ],
      "metadata": {
        "id": "y8TxYWgGSmdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# hyperparamters\n",
        "\n",
        "VOCAB_SIZE =len(vocab)    # from your custom tokenizer\n",
        "D_MODEL = 256             # embedding dimension\n",
        "SEQ_LEN = 128             #sequence length\n",
        "\n",
        "# token embedding layer\n",
        "\n",
        "token_embedding = nn.Embedding(VOCAB_SIZE, D_MODEL)\n",
        "\n",
        "# example\n",
        "\n",
        "sample_batch = torch.tensor(train_encoded[:2])\n",
        "embeddings = token_embedding(sample_batch)\n",
        "print(\"embedding \",embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCZhF9dLTKTj",
        "outputId": "021f92ae-cf2e-4691-813c-bcff2d529914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedding  torch.Size([2, 128, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Positional Encoding*"
      ],
      "metadata": {
        "id": "kQVZC4w5WBEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=SEQ_LEN):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model) # tensor of zeros len of max len and dimension\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # adding one more dimension\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # calculating the 100002i/dmodel\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)   # even indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)   # odd indices\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # shape [1, max_len, d_model]\n",
        "        self.register_buffer('pe', pe)  # not learnable, just stored\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch, seq_len, d_model]\n",
        "        return x + self.pe[:, :x.size(1)]\n"
      ],
      "metadata": {
        "id": "cmxctzT8WDYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_encoder = PositionalEncoding(D_MODEL, SEQ_LEN)\n",
        "\n",
        "#pass through token embedding + PE\n",
        "x = token_embedding(sample_batch)\n",
        "x = pos_encoder(x)\n",
        "\n",
        "print(\"with PE shape\", x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqX6VgWlXeUL",
        "outputId": "918863d4-aed9-45de-d3d0-92040d529780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "with PE shape torch.Size([2, 128, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IOUb28rjX8zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model Architecture**"
      ],
      "metadata": {
        "id": "DkAfqkL3EE2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class MaskedMultiHeadSelfAttention(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, dropout=0.0):\n",
        "    super().__init__()\n",
        "    assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = embed_dim // num_heads # integer division or reverse\n",
        "    self.scale = 1.0 / math.sqrt(self.head_dim) # sqrtroot of head_dim for scaled dot product\n",
        "\n",
        "      #in projetcion for Q K V (combined to gain better speed )\n",
        "    self.qkv_proj = nn.Linear(embed_dim, 3*embed_dim, bias=True)\n",
        "    # out projection\n",
        "    self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def _causal_mask(self, seq_len, device):\n",
        "    # lower triangular matrix with 1s on and below diagonal\n",
        "    mask = torch.tril(torch.ones((seq_len, seq_len), device=device)).unsqueeze(0).unsqueeze(0)\n",
        "    # torch.onces metrix full of ones\n",
        "    # torch tril lower triangular will become 1s rest 0s\n",
        "    # unsquees 2 times will add 2 new dimension  1,1,seq_len,seq_len\n",
        "    # convert to additive mask 0 for allowed -1e9 for disallowed\n",
        "    return (1.0-mask) * -1e9   # if mask = 1 = 1.0-1 = 0 * -1.e9 = 0 and 1.0-0 = 1 * -1e9 = -1e9\n",
        "\n",
        "    #tensor([[[[0., -1e9, -1e9, -1e9],\n",
        "          #[0.,    0., -1e9, -1e9],\n",
        "          #[0.,    0.,    0., -1e9],\n",
        "          #[0.,    0.,    0.,    0.]]]])\n",
        "\n",
        "  def forward(self, x,key_padding_mask=None, attn_mask=None):\n",
        "\n",
        "    \"\"\"\n",
        "        x: (batch, seq_len, embed_dim)\n",
        "        key_padding_mask: optional (batch, seq_len) with 1 for tokens to KEEP and 0 for PAD tokens (or vice versa - doc below)\n",
        "            We'll assume mask==1 means keep; if you have mask with True for padding you can invert it.\n",
        "        attn_mask: optional additive mask of shape (seq_len, seq_len) or (batch, seq_len, seq_len)\n",
        "            If provided, it will be added to attention scores (like causal mask), before softmax.\n",
        "        Returns:\n",
        "            out: (batch, seq_len, embed_dim)\n",
        "            attn_weights: (batch, num_heads, seq_len, seq_len)  -- optional, helpful for debugging\n",
        "        \"\"\"\n",
        "    B,T,C = x.shape # b = batch size , t is token seq len , and c is embedding dimension channel\n",
        "\n",
        "    device = x.device\n",
        "\n",
        "    # project to qkv and split\n",
        "    qkv = self.qkv_proj(x) # x as input for qkv_proj embeddingdim\n",
        "    qkv = qkv.reshape(B,T,3,self.num_heads, self.head_dim) # spliting qkv into B,T AND 3*embed into 3 * num_heads * head_dim\n",
        "    qkv = qkv.permute(2,0,3,1,4) # (3,B,num_heads,t,head)\n",
        "    q,k,v = qkv[0], qkv[1], qkv[2] # each shape (b, num_heads, T, head_dim) saparate tensor q,k,v  for all heads\n",
        "\n",
        "\n",
        "    # computing the scaled dot product QKV = softmax(QKt/rootDk) V\n",
        "\n",
        "    scores = (q @ k.transpose(-2,-1)) * self.scale # q @ k multiply * scale (ie 1/rootDK) =  scores which has (B, num_head, T, T) ie t-query and t-key\n",
        "    #causal mask\n",
        "    causal = self._causal_mask(T,device)\n",
        "    #scores has shape (B, num_heads, T, T) — the dot products of Q and K.\n",
        "    # T is the query length and T is key len\n",
        "    scores = scores + causal\n",
        "\n",
        "\n",
        "    # optional user provided attn_mask\n",
        "    if attn_mask is not None:\n",
        "      scores = scores + attn_mask\n",
        "\n",
        "\n",
        "    if key_padding_mask is not None: # IF IT IS NOT NONE\n",
        "      if key_padding_mask.dtype == torch.bool: # check bool? 1 or 0 . ~ this opreator inverts the true to false\n",
        "        keep_mask = (~key_padding_mask).to(dtype=torch.float32) # invert the boolean mask 1 to 0 , 0 to 1 keep. and also convert it to float\n",
        "      else: # if it is not bool\n",
        "        keep_mask = key_padding_mask.to(dtype=torch.float32)  # directly converting it 1 to keep and 0 to ignore\n",
        "\n",
        "      keep_mask = keep_mask.unsqueeze(1).unsqueeze(1) # Adds two singleton dimensions so it broadcasts across batch and heads: (B, 1, 1, T)\n",
        "      scores = scores + (1.0 - keep_mask) * -1e9  # 1.0 - 1 = 0 * -1e9 = 0 + scores\n",
        "      # 1.0-0 = 1 *-1e9 + scores\n",
        "\n",
        "     # attention probabilities\n",
        "    attn = F.softmax(scores, dim=-1)  # (B, num_heads, T, T) applying softmax\n",
        "    attn = self.dropout(attn) # dropouts\n",
        "\n",
        "        # attention output\n",
        "    out = torch.matmul(attn, v)  # matrix multiplication attn * v\n",
        "        # combine heads\n",
        "    out = out.transpose(1, 2).contiguous().view(B, T, C)  # out shape before: (B, num_heads, T, head_dim) = transpose(1, 2) → (B, T, num_heads, head_dim)\n",
        "    # .view(B, T, C) → flatten num_heads * head_dim = embed_dim\n",
        "    out = self.out_proj(out)  # (B, T, embed_dim)\n",
        "\n",
        "    return out, attn  # return attn if you want to inspect i\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sV_bzBURE-bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Tester*"
      ],
      "metadata": {
        "id": "Pw81r5TinOUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparams\n",
        "batch = 2\n",
        "seq_len = 16\n",
        "embed_dim = 128\n",
        "num_heads = 8\n",
        "\n",
        "mha = MaskedMultiHeadSelfAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=0.1)\n",
        "x = torch.randn(batch, seq_len, embed_dim)   # token embeddings + pos encodings\n",
        "\n",
        "out, attn = mha(x)\n",
        "print(out.shape)   # -> (2, 16, 128)\n",
        "print(attn.shape)  # -> (2, 8, 16, 16)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgQ7elPU4MwK",
        "outputId": "af88d76b-18e3-4f52-9145-9b50f7d1327e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 16, 128])\n",
            "torch.Size([2, 8, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "          ┌───────────────┐\n",
        "          │ Input x       │\n",
        "          └───────┬───────┘\n",
        "                  │\n",
        "                  ▼\n",
        "     ┌─────────────────────────┐\n",
        "     │ Masked Multi-Head Attn  │\n",
        "     └─────────┬───────────────┘\n",
        "               │\n",
        "               ▼\n",
        "        Dropout + Residual\n",
        "               │\n",
        "               ▼\n",
        "           LayerNorm\n",
        "               │\n",
        "               ▼\n",
        "          Feed-Forward\n",
        "               │\n",
        "               ▼\n",
        "        Dropout + Residual\n",
        "               │\n",
        "               ▼\n",
        "           LayerNorm\n",
        "               │\n",
        "               ▼\n",
        "           Output x\n"
      ],
      "metadata": {
        "id": "y1VK97cRnQ9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Decoder Block*"
      ],
      "metadata": {
        "id": "p0Lmhc8nslT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(d_model, d_ff)\n",
        "    self.linear2 = nn.Linear(d_ff, d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.activation = nn.GELU() #GELU (Gaussian Error Linear Unit)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.linear2(x)\n",
        "    return x\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self , d_model, num_heads, d_ff, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.mha = MaskedMultiHeadSelfAttention(d_model, num_heads, dropout)\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.ffn = FeedForward(d_model, d_ff, dropout)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # masked multihead self attention + residual\n",
        "    attn_out, _ = self.mha(x)\n",
        "    x = self.norm1(x+self.dropout(attn_out))\n",
        "\n",
        "    ffn_out = self.ffn(x)\n",
        "    x = self.norm2(x+self.dropout(ffn_out))\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "eZ9KxKN-507w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b , s , d = 2,16,128\n",
        "n_heads , d_ff = 8 , 512\n",
        "\n",
        "decoder_block = DecoderBlock(d,n_heads,d_ff)\n",
        "x = torch.randn(b,s,d)\n",
        "out = decoder_block(x)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWD0Ux2rpa8O",
        "outputId": "c212c40a-443b-4dff-8b3c-5d5f43331404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 16, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**miniGPT**\n"
      ],
      "metadata": {
        "id": "Tv9AVaott29-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MiniGPT(nn.Module):\n",
        "  def __init__(self, vocab_size, seq_len, d_model=256, num_heads=8, d_ff=512, num_layers=6, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.seq_len= seq_len\n",
        "    self.d_model = d_model\n",
        "\n",
        "\n",
        "    # token embedding\n",
        "\n",
        "    self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "    # positional encoding\n",
        "    self.pos_encoder = PositionalEncoding(d_model, max_len=seq_len)\n",
        "\n",
        "    #stack of decoder\n",
        "    self.layers = nn.ModuleList([DecoderBlock(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "    # output projectiobn\n",
        "    self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "  def forward(self, input_ids):\n",
        "\n",
        "  # embedding + positional encoding\n",
        "\n",
        "    x = self.token_embedding(input_ids)\n",
        "    x = self.pos_encoder(x)\n",
        "\n",
        "# pass through all decoder blovks\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "\n",
        "  # output logits\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g66ZouF0t2zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ocrgR4Owajc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Loop**"
      ],
      "metadata": {
        "id": "X_P-cU_V5oUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Define loss optimizer*"
      ],
      "metadata": {
        "id": "h1ZbVrwk6FSY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*training loop*"
      ],
      "metadata": {
        "id": "UqY1l65h6Ws-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G8a0-ud80S4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "SEQ_LEN = 128\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 5\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/mini_gpt_checkpoints\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "scaler = torch.cuda.amp.GradScaler()  # for mixed precision\n",
        "model = MiniGPT(vocab_size=vocab_size, seq_len=SEQ_LEN  )\n",
        "model.to(DEVICE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "scaler = torch.cuda.amp.GradScaler()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K61uJlulq9YP",
        "outputId": "219f91f3-436e-42f2-b4aa-b66dc4bd1621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-27342905.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()  # for mixed precision\n",
            "/tmp/ipython-input-27342905.py:15: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import math, os\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_epoch(dataloader):\n",
        "    model.train()\n",
        "    total_loss, total_tokens = 0, 0\n",
        "    for batch in tqdm(dataloader):\n",
        "        input_ids = batch[0].to(DEVICE)\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "            logits = model(input_ids)\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item() * input_ids.numel()\n",
        "        total_tokens += input_ids.numel()\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    ppl = math.exp(avg_loss)\n",
        "    return avg_loss, ppl\n",
        "\n",
        "def eval_epoch(dataloader):\n",
        "    model.eval()\n",
        "    total_loss, total_tokens = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[0].to(DEVICE)\n",
        "            labels = input_ids.clone()\n",
        "            logits = model(input_ids)\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "\n",
        "            total_loss += loss.item() * input_ids.numel()\n",
        "            total_tokens += input_ids.numel()\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    ppl = math.exp(avg_loss)\n",
        "    return avg_loss, ppl\n"
      ],
      "metadata": {
        "id": "JoSkoTWE6VqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "best_val_ppl = float(\"inf\")\n",
        "latest_checkpoint = os.path.join(CHECKPOINT_DIR, \"mini_gpt_best.pt\")\n",
        "\n",
        "if os.path.exists(latest_checkpoint):\n",
        "    print(\"✅ Loading checkpoint to resume training...\")\n",
        "    checkpoint = torch.load(latest_checkpoint)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "    start_epoch = checkpoint[\"epoch\"]\n",
        "    best_val_ppl = checkpoint[\"val_perplexity\"]\n",
        "\n",
        "# --- 6️⃣ Training loop ---\n",
        "for epoch in range(start_epoch, NUM_EPOCHS):\n",
        "    train_loss, train_ppl = train_epoch(train_loader)\n",
        "    val_loss, val_ppl = eval_epoch(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train PPL: {train_ppl:.2f} | Val PPL: {val_ppl:.2f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_ppl < best_val_ppl:\n",
        "        best_val_ppl = val_ppl\n",
        "        torch.save({\n",
        "            \"epoch\": epoch+1,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"val_perplexity\": val_ppl\n",
        "        }, os.path.join(CHECKPOINT_DIR, \"mini_gpt_best.pt\"))\n",
        "        print(f\"✅ Saved new best model at epoch {epoch+1} with Val PPL: {val_ppl:.2f}\")"
      ],
      "metadata": {
        "id": "zW4sU1G46cbs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f99711f7-fed6-46f7-eceb-1193688162a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20359/20359 [13:00<00:00, 26.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 | Train PPL: 1.60 | Val PPL: 1.19\n",
            "✅ Saved new best model at epoch 1 with Val PPL: 1.19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20359/20359 [12:50<00:00, 26.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5 | Train PPL: 1.19 | Val PPL: 1.17\n",
            "✅ Saved new best model at epoch 2 with Val PPL: 1.17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20359/20359 [12:47<00:00, 26.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5 | Train PPL: 1.18 | Val PPL: 1.16\n",
            "✅ Saved new best model at epoch 3 with Val PPL: 1.16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20359/20359 [12:56<00:00, 26.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5 | Train PPL: 1.17 | Val PPL: 1.17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20359/20359 [12:38<00:00, 26.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5 | Train PPL: 1.18 | Val PPL: 1.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MiniGPT**"
      ],
      "metadata": {
        "id": "5w-CwvjXANk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def generate_text(model, start_tokens, inv_vocab, max_len=100, temperature=1.0):\n",
        "    \"\"\"\n",
        "    model: your trained MiniGPT decoder\n",
        "    start_tokens: list of token IDs as prompt\n",
        "    inv_vocab: {id: token} mapping\n",
        "    max_len: number of tokens to generate\n",
        "    temperature: randomness, 1.0 = default, <1 = more confident, >1 = more random\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    generated = start_tokens.copy()\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        # take last SEQ_LEN tokens as input\n",
        "        input_ids = torch.tensor([generated[-SEQ_LEN:]], device=DEVICE)\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_ids)  # [1, seq_len, vocab_size]\n",
        "            next_token_logits = logits[0, -1] / temperature\n",
        "            probs = torch.softmax(next_token_logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
        "        generated.append(next_token)\n",
        "\n",
        "    # convert token IDs back to text\n",
        "    text = \" \".join([inv_vocab.get(tok, \"<unk>\") for tok in generated])\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "AR8nlI1CARd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose you have a start prompt like \"The future of AI\"\n",
        "prompt_text = \"The future of Tech\"\n",
        "start_tokens = tokenize_text(prompt_text, vocab)  # your existing tokenization function\n",
        "\n",
        "# Generate text\n",
        "generated_text = generate_text(model, start_tokens, inv_vocab, max_len=50, temperature=0.8)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtcZ2uvcFRB9",
        "outputId": "cefacd8f-3f0c-44fc-f3e2-8f23d1d1ce8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the future of tech edges 588 onondaga eponymous homework android 128 flee masques harrington rocco perceptions unitis devastated bohr alcoholics winning surrounded birbhum yip mcdiarmid detached takahashi kids strains nullified sensors goa scelidosaurus virginia cicada iximche platte barracudas 1364 prelate valdés mcdermott persevered bgsu domestication magnificat inclusive laid vulnerabilities vishal prefix damacy dicaprio maeda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install huggingface_hub\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17_ia41HFTVL",
        "outputId": "ff83df67-828b-40b8-90e0-9e25e7b6ed4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.35.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c8t2Q6GdJlNm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}