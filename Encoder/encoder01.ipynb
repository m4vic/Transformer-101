{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1WO08FmlyanPTOQGuKCFx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m4vic/Transformer-101/blob/main/encoder01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzPwByRWz7SK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# imports\n",
        "import math, random\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8veY3B6Ez-Xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***DataLoading***"
      ],
      "metadata": {
        "id": "odATjYu-Lk_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw  = load_dataset(\"ag_news\")\n",
        "\n",
        "train_raw = raw[\"train\"]\n",
        "test_raw = raw[\"test\"]\n",
        "\n",
        "train_split = train_raw.train_test_split(test_size=0.2, seed=42)\n",
        "train_raw = train_split[\"train\"]\n",
        "val_raw = train_split[\"test\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306,
          "referenced_widgets": [
            "eee3029f42fb482f90e193c81658b5c9",
            "e6661f8b894346b488b6d8c3e6a2d7b7",
            "8050ece011184c35baf7a7eb13a38369",
            "dd457a00d70f496eaa312c467c6ace3e",
            "5c6e2652bfd046579b9e58cb761ab088",
            "dfc51d0432724296aa186de6dd9507b0",
            "d4592e89bf494b6bac4db73631971293",
            "d0a0a031ed0e43fda72c3dc3d2c1a49d",
            "43c0b416f2fd4b54988132128cd9ce3a",
            "5816fe7255fb41e48244e70876d52c5c",
            "b2acab26221f47ac92e8c8b32be5b268",
            "1fc5b81a84e141d3805bf0e639653003",
            "0da1fb27be7845ba9f7f57e8817c4d1e",
            "950b7d20528c49cf90b33fe509c70fe5",
            "a6e25b22745540cdbebda5fcdb3ff9a9",
            "4961ad07b47e4feeb094517e2991b900",
            "17eb7e1b81014ba4acbfa9b03d36e737",
            "4b481ac5bf784d5c8bb10c71a633f424",
            "77f0a5530a81401887399163bcfb5eef",
            "6ade006693f8426cb0a1a93ed34d064d",
            "44ede6db980c462b977d2f911ba2be3b",
            "8ebae328cd45488c90e831367b216c99",
            "9056968078b14ded95ad0f6ef7298484",
            "ccb4807ab01f44ceac514967845d91f0",
            "64dfbe027a0d43c49bb7aaba45ce3aac",
            "6544e29e5dd041f5bc9edc84f1b81278",
            "13917bee4d3f48c7bce63babd569679a",
            "2734f95ab8974adda5907663ab76b54d",
            "67979aaca6554606926e21d7eae3ca1f",
            "b17fd01ae687411c95b831122add756e",
            "28fc6107f1d541368f5f6943b18f9e61",
            "2c687720e9754151a94e895c70309598",
            "a984f5fdbf524884b4428213eab239d8",
            "af6f9c921b264c95af29e9663ffae006",
            "1c207e7fedbc4ce282405c33bfba3bef",
            "d01c7c4bb6874169a4f2f9dfd67a4c51",
            "f3d08df453f64167a430af514a59a382",
            "068b06a5b3984889a5beea0d83ef0ae6",
            "568dc7fea41b48859c0bf1d54ea331c8",
            "9849a5b1c7924b598002ac220a5e232f",
            "1553568fe7ba4624834cd9419f53ae0f",
            "ebb7302760e84de79b1e4a96055b0b2a",
            "528ca5dbef5540af98a88f55c83fea5d",
            "d064ceb2a19849aaa899c40c440bce12",
            "b9306bfbdeda4dd7be21fbd93eea4db9",
            "6c0c26fdd561417ea178443c3607170e",
            "7f05664f98594639a0d47d4062746016",
            "c946223a3706497daa14d2efc47bce50",
            "57b09000271e4cc69fc1495d3d8c7284",
            "ebf851b9ebf94b6b8e7f53a2a687d217",
            "57ed164e3ab248a7bc956a7b731eaab5",
            "af0a33f76238432691867c7d77a1992f",
            "fe1e33ec51b942a18db600967f2b42a8",
            "234ab953a13c459a8fb371bdf86fc2cc",
            "72dbef8edcd24f739db05a1021f7f33e"
          ]
        },
        "id": "6c4QE8tKLP7G",
        "outputId": "dc50ea45-8fbb-4b1b-ba5e-b255123570e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eee3029f42fb482f90e193c81658b5c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1fc5b81a84e141d3805bf0e639653003"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9056968078b14ded95ad0f6ef7298484"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af6f9c921b264c95af29e9663ffae006"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9306bfbdeda4dd7be21fbd93eea4db9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print first few raw samples\n",
        "for i in range(10):\n",
        "    print(f\"Text {i}: {train_raw[i]['text']}\")\n",
        "    print(f\"Label {i}: {train_raw[i]['label']}\")\n",
        "    print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oayakt9WLtC2",
        "outputId": "6d571540-9095-4be6-965c-ea11321f7859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 0: Nation #39;s Cotton Crop May Exceed Records This year #39;s cotton crop is on pace to be the largest in US history, although hurricanes that have battered the nation in the past few months may reduce the final amount.\n",
            "Label 0: 2\n",
            "================================================================================\n",
            "Text 1: 18 years and still rollin #39; ALEX FERGUSON will take up the one-year rolling option on his contract and continue as Manchester United boss next season. Ferguson celebrates 18 years in charge at Old Trafford in the Manchester derby tomorrow.\n",
            "Label 1: 1\n",
            "================================================================================\n",
            "Text 2: Madrid Masters: Safin beats Nalbandian Sunday #39;s final of the Madrid Masters pitted players ranked ninth, Marat Safin, and 10th, David Nalbandian, in the world; both near the top of their games.\n",
            "Label 2: 1\n",
            "================================================================================\n",
            "Text 3: Sirius Satellite Signs Howard Stern to 5-Year Accord (Update9) Howard Stern, host of the top-rated radio show for young men in New York and Los Angeles, will move to Sirius Satellite Radio Inc.\n",
            "Label 3: 2\n",
            "================================================================================\n",
            "Text 4: NATO, Russia To Meet Over Beslan School Siege 6 September 2004 -- NATO Secretary-General Jaap de Hoop Scheffer has called a special meeting of NATO countries and Russia for tomorrow to discuss the school siege in southern Russia, in which more than 330 people died.\n",
            "Label 4: 3\n",
            "================================================================================\n",
            "Text 5: WTO Rules Against EU Protection of Goods (AP) AP - The United States and Australia prevailed in an interim ruling by the World Trade Organization in a dispute over protection given by the European Union to its regional goods such as Champagne wine and Feta cheese, trade officials said Thursday.\n",
            "Label 5: 0\n",
            "================================================================================\n",
            "Text 6: A Low-Key Migration (washingtonpost.com) washingtonpost.com - Change is coming for AT T Wireless's 21.7 million customers -- although they wouldn't know it from watching the company's commercials or shopping in its stores.\n",
            "Label 6: 3\n",
            "================================================================================\n",
            "Text 7: Google Plans Desktop Search Tool for Apple PCs Google Inc. (GOOG.O: Quote, Profile, Research) plans to release a version of its desktop search tool for computers running on the Mac operating system from Apple Computer Inc.\n",
            "Label 7: 3\n",
            "================================================================================\n",
            "Text 8: Security tightened in Madhya Pradesh #39;s Chambal belt Security has been tightened and alert sounded in entire Chambal belt of Madhya Pradesh on Saturday following massacre of 13 villagers by the dreaded Gadaria dacoit gang, while authorities were discussing new strategies to check the gang #39;s activities.\n",
            "Label 8: 0\n",
            "================================================================================\n",
            "Text 9: Aussie equestrian hopes end in sixth Australia #39;s dreams of an historic fourth successive three-day eventing gold medal ended in disappointment but there was still joy for the team when Andrew Hoy #39;s wife won dual gold medals. \n",
            "Label 9: 1\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uOsEhqWALukt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ll1Qy8pEGAW"
      },
      "source": [
        "# **Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHQDNbOpH40B"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UtODnWIEFy4"
      },
      "outputs": [],
      "source": [
        "# imorting r and collections\n",
        "import re\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4j4ZmqxPDRPF"
      },
      "outputs": [],
      "source": [
        "# function for tokenization\n",
        "\n",
        "def tokenize(text):\n",
        "    text = text.lower()\n",
        "    # match either words (\\w+) OR single punctuation ([^\\w\\s])\n",
        "    return re.findall(r\"\\w+|[^\\w\\s]\", text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2zWqXkngQ4b",
        "outputId": "db723cf1-5ab8-4bff-b9cb-9a6d372fd572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hi', 'my', 'name']\n"
          ]
        }
      ],
      "source": [
        "print(tokenize(\"hi my name\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMhaNu8DBZUr"
      },
      "outputs": [],
      "source": [
        "# defining the function counter\n",
        "counter = Counter()\n",
        "\n",
        "# counter\n",
        "for ex in train_raw: # for string in dataset train sentence\n",
        "  counter.update(tokenize(ex[\"text\"])) #  update in counter tuple . by tokenizing all the sentences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cttg9ow7H_DN"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "29msJX9ECnZ1"
      },
      "outputs": [],
      "source": [
        "# adding special tokens\n",
        "vocab_size = 30000\n",
        "\n",
        "most_common = counter.most_common(vocab_size-3) # reserve 3 tokens\n",
        "\n",
        "itos = ['<PAD>','<UNK>','<CLS>'] + [w for w,_ in most_common]\n",
        "stoi = {w:i for i,w in enumerate(itos)}\n",
        "\n",
        "PAD_IDX = stoi['<PAD>']; UNK_IDX = stoi['<UNK>']; CLS_IDX = stoi['<CLS>']\n",
        "\n",
        "def encode_text(text, max_len=256):\n",
        "  tokens = tokenize(text)[:max_len-1]\n",
        "  ids = [CLS_IDX] + [stoi.get(t, UNK_IDX) for t in tokens]\n",
        "  return ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9shTY7dA4RxD",
        "outputId": "ec8f310c-445e-4be4-efec-d5d74ca3efdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['nation', '#', '39', ';', 's', 'cotton', 'crop', 'may', 'exceed', 'records', 'this', 'year', '#', '39', ';', 's', 'cotton', 'crop', 'is', 'on', 'pace', 'to', 'be', 'the', 'largest', 'in', 'us', 'history', ',', 'although', 'hurricanes', 'that', 'have', 'battered', 'the', 'nation', 'in', 'the', 'past', 'few', 'months', 'may', 'reduce', 'the', 'final', 'amount', '.']\n"
          ]
        }
      ],
      "source": [
        "print(tokenize(train_raw[0][\"text\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPgBOSdQCx4H"
      },
      "outputs": [],
      "source": [
        "class agnewsDataset(Dataset):\n",
        "    def __init__(self, hf_dataset, max_len=256):\n",
        "\n",
        "        self.examples = hf_dataset\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self): return len(self.examples) # return len of eg\n",
        "\n",
        "    def __getitem__(self, idx): #returns the tensor of ids and labels\n",
        "        text = self.examples[idx]['text'] # saves text\n",
        "        label = int(self.examples[idx]['label']) # saves label\n",
        "        ids = encode_text(text, self.max_len) # indices of text\n",
        "        return torch.tensor(ids, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "def collate_batch(batch):\n",
        "    ids, labels = zip(*batch) # unpacls and zips\n",
        "    lengths = [len(x) for x in ids] # len of x\n",
        "    maxl = max(lengths) # the max value\n",
        "    padded = torch.full((len(ids), maxl), PAD_IDX, dtype=torch.long) # paddded is a tensor of len of ids and max len of maxl\n",
        "    attn_mask = torch.zeros((len(ids), maxl), dtype=torch.long) #this is tensor of zero\n",
        "    for i, x in enumerate(ids):\n",
        "        padded[i, :len(x)] = x\n",
        "        attn_mask[i, :len(x)] = 1\n",
        "    return padded, attn_mask, torch.tensor(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q4FdtMWIaXP"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_UsZczrj_PT"
      },
      "source": [
        "# **Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZDYGKJfIaBM"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "\n",
        "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        pe = pe.unsqueeze(0)   # (1, max_len, d_model)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, d_model)\n",
        "        seq_len = x.size(1)\n",
        "        return self.pe[:, :seq_len, :]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KWHbsE4IS5N"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask=None, dropout=None):\n",
        "    # q,k,v: (batch, n_heads, seq_len, head_dim)\n",
        "    dk = q.size(-1)\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(dk)   # (batch, n_heads, seq, seq)\n",
        "    if mask is not None:\n",
        "        # mask: (batch, 1, 1, seq) or (batch, 1, seq, seq)\n",
        "        scores = scores.masked_fill(mask == 0, float('-1e9'))\n",
        "    attn = F.softmax(scores, dim=-1)\n",
        "    if dropout is not None:\n",
        "        attn = dropout(attn)\n",
        "    output = torch.matmul(attn, v)   # (batch, n_heads, seq, head_dim)\n",
        "    return output, attn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0pxh-PR9sZRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODrmxJSRIt5j"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0  # checks that mod == 0\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads # integer devison\n",
        "\n",
        "        self.q_lin = nn.Linear(d_model, d_model) # init nnLinear layer\n",
        "        self.k_lin = nn.Linear(d_model, d_model)\n",
        "        self.v_lin = nn.Linear(d_model, d_model)\n",
        "        # final linear layer\n",
        "        self.out_lin = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # x: (batch, seq, d_model)\n",
        "        batch, seq, _ = x.size()\n",
        "\n",
        "        q = self.q_lin(x).view(batch, seq, self.n_heads, self.head_dim).transpose(1,2) # (batch, heads, seq, head_dim)\n",
        "        k = self.k_lin(x).view(batch, seq, self.n_heads, self.head_dim).transpose(1,2) # transposing 1 to 2 means sek will become heads\n",
        "        v = self.v_lin(x).view(batch, seq, self.n_heads, self.head_dim).transpose(1,2) # .view devides the d_model to n_heads to get head_dim\n",
        "\n",
        "\n",
        "\n",
        "        if mask is not None:\n",
        "            # mask: (batch, seq) -> make (batch, 1, 1, seq)\n",
        "            mask = mask.unsqueeze(1).unsqueeze(1)\n",
        "\n",
        "\n",
        "        attn_output, attn = scaled_dot_product_attention(q,k,v,mask, self.dropout) # scaled dot product\n",
        "        # attn_output: (batch, heads, seq, head_dim)\n",
        "        attn_output = attn_output.transpose(1,2).contiguous().view(batch, seq, self.d_model) # it re shapes it first transpose and thent by using\n",
        "        #.view we multiply n_head * head_dim = d_model\n",
        "\n",
        "        return self.out_lin(attn_output), attn  # return (batch, seq, d_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAf-p3KSJBma"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, dim_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim_ff, d_model),\n",
        "        )\n",
        "    def forward(self, x): return self.net(x) # applying all the layer and give x as output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwY7H7VbI6Ek"
      },
      "source": [
        "creating the encoder function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feBPp8pBJE1G"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dim_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # declaring mha,nlayernorm,ff,etc\n",
        "        self.mha = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ff = FeedForward(d_model, dim_ff, dropout)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None): # encoder forward pass\n",
        "        # x: (batch, seq, d_model)\n",
        "\n",
        "        attn_out, _ = self.mha(x, mask) # calling mha and returning attn out\n",
        "        x = x + self.dropout(attn_out) # applying reidual + dropout  + attn out\n",
        "        x = self.norm1(x) # layer norm\n",
        "        ff_out = self.ff(x) # ff\n",
        "        x = x + self.dropout(ff_out)\n",
        "        x = self.norm2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlYO-NDeJVvn"
      },
      "source": [
        "testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDrggzdcQjnq"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, n_heads=4, num_layers=2, dim_ff=512, max_len=256, dropout=0.1, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # embedding layer\n",
        "        self.tok_embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        # positional encoding\n",
        "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        # model\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, dim_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "      # applying embedding + deviding with sqrt of d_model\n",
        "        x = self.tok_embed(input_ids) * math.sqrt(self.d_model)\n",
        "      # positional encoding + input\n",
        "        x = x + self.pos_enc(x)\n",
        "      # dropout\n",
        "        x = self.dropout(x)\n",
        "      # model application and saving it to x\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, attention_mask)\n",
        "        return x   # (batch, seq, d_model)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSrsMqiAQxZd"
      },
      "outputs": [],
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, encoder, d_model, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.head = nn.Linear(d_model, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        enc_out = self.encoder(input_ids, attention_mask) # calling the encoder\n",
        "        cls = (enc_out * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1, keepdim=True)       # we added CLS at position 0 in tokenizer\n",
        "        logits = self.head(cls)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18hbtjgaJpnV"
      },
      "source": [
        "testing the collate function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOu5mkhLkQtl"
      },
      "source": [
        "# **Training Loop**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "parameters -\n",
        "vocab_size = 30_000\n",
        "d_model = 256\n",
        "n_heads = 8\n",
        "num_layers = 4\n",
        "dim_ff = 512\n",
        "num_classes = 4\n",
        "max_len = 256\n",
        "\n"
      ],
      "metadata": {
        "id": "2r4hHzupBDiQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygI2xUpKkQXC"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# instantiate\n",
        "vocab_size = len(itos)  # if using scratch tokenizer\n",
        "encoder = TransformerEncoder(vocab_size, d_model=256, n_heads=8, num_layers=4, dim_ff=512, max_len=256, pad_idx=PAD_IDX, dropout=0.2)\n",
        "model = SentimentClassifier(encoder, d_model=256, num_classes=4).to(device)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "D4Eeyh8mBOJs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MiecY25APCu"
      },
      "outputs": [],
      "source": [
        "def train_epoch(dataloader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "\n",
        "\n",
        "    for input_ids, attn_mask, labels in tqdm(dataloader):\n",
        "\n",
        "        input_ids = input_ids.to(device); attn_mask=attn_mask.to(device); labels=labels.to(device) # shifting to gpu\n",
        "\n",
        "\n",
        "        logits = model(input_ids, attn_mask) # orward pass\n",
        "\n",
        "        loss = criterion(logits, labels) # loss calculation\n",
        "\n",
        "        optimizer.zero_grad() # optmizer making gradient 0\n",
        "\n",
        "        loss.backward() # backword pass\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # preventing exploding gradient\n",
        "\n",
        "        optimizer.step() # optimizing\n",
        "\n",
        "        total_loss += loss.item() * input_ids.size(0) # calculating loss\n",
        "\n",
        "        preds = logits.argmax(dim=1) # prediction\n",
        "\n",
        "        correct += (preds == labels).sum().item()\n",
        "\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return total_loss/total, correct/total\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sU2yvttzATjZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def eval_epoch(dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    correct = 0; total=0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for input_ids, attn_mask, labels in dataloader:\n",
        "\n",
        "            input_ids = input_ids.to(device); attn_mask=attn_mask.to(device); labels=labels.to(device)\n",
        "\n",
        "            logits = model(input_ids, attn_mask)\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            total_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "            preds = logits.argmax(dim=1)\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "\n",
        "            total += labels.size(0)\n",
        "    return total_loss/total, correct/total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_CqYrIp-t-T"
      },
      "outputs": [],
      "source": [
        " import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "fb9103811ea74b6bab76560b36703f3f",
            "ad7516a18c5740239e35f258262d058d",
            "6f18f41174914a95a5575534a88052c2",
            "a2341f2511c64ff2a0f0a10c674a2462",
            "ac883a49b7564e03af4675b2b9241ac6",
            "0c1baf0bdf3d48cab2fffff675c2d8b6",
            "eb51d6a484384c598e6cc4a554b96b31",
            "2adf46f2ab34457b800f13c44ab8c771",
            "9ec077afe7e747f1b56e8288d5cdf627",
            "cd17ed08b8a74e6b8db00eda9d76500c",
            "9c7a2e4ec82248c5848caa778c195e2c",
            "73b0d3f6d1e349f9b09c6121620e6e97",
            "57e71852a68f4a9da064dbacfad81b88",
            "53217902b11446fc8cf8efd6b320a1f7",
            "2a9d1265ad5f4d2387a87b7f6bbd2c21",
            "0c5b367c6f1d463ebab41a5db6ec0378",
            "015268ee98864a87b8abb77fb2c341ec",
            "f92911f3baf046f783f367a555a26c63",
            "6529c5a16ec0422aa7cb6b22e5cd99db",
            "43beba22b2904f9fa7c1d2cb81aabcdf",
            "e2f5f95abe584e5e90a606d83857a06c",
            "bebbfabf0ff949d7bc059f4dfdf2dc5d"
          ]
        },
        "id": "p42LOQLekhqC",
        "outputId": "ade7b1e3-981d-46c7-98ca-305ba7f95305"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb9103811ea74b6bab76560b36703f3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14: train_loss=0.2305 train_acc=0.9213 | val_loss=0.2654 val_acc=0.9175\n",
            "✅ Saved new best model at epoch 14 with val_acc=0.9175\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73b0d3f6d1e349f9b09c6121620e6e97"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15: train_loss=0.2288 train_acc=0.9216 | val_loss=0.2855 val_acc=0.9170\n"
          ]
        }
      ],
      "source": [
        "train_ds = agnewsDataset(train_raw, max_len=256)\n",
        "val_ds   = agnewsDataset(val_raw, max_len=256)\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
        "val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "best_val_acc = 0.0\n",
        "for epoch in range(14, 16):\n",
        "    train_loss, train_acc = train_epoch(train_loader)\n",
        "    val_loss, val_acc = eval_epoch(val_loader)\n",
        "    print(f\"Epoch {epoch}: train_loss={train_loss:.4f} train_acc={train_acc:.4f} | val_loss={val_loss:.4f} val_acc={val_acc:.4f}\")\n",
        "    # save best\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "\n",
        "\n",
        "        torch.save({\n",
        "      \"epoch\": epoch,\n",
        "      \"model_state_dict\": model.state_dict(),\n",
        "      \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "      \"val_acc\": val_acc\n",
        "      }, \"agnews_encoder.pt\")\n",
        "\n",
        "        # save vocab\n",
        "        with open(\"vocab.json\", \"w\") as f:\n",
        "            json.dump({\"stoi\": stoi, \"itos\": itos}, f)\n",
        "\n",
        "        print(f\"✅ Saved new best model at epoch {epoch} with val_acc={val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_enSoIUJxzj"
      },
      "source": [
        "MODEL ARCHITECTURE\n",
        "\n",
        "> Input (B, 64) with pad_mask to ignore the <pad>\n",
        "> Embedding LAyer dimension 128\n",
        "> positonal encoding\n",
        ">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Prediction**"
      ],
      "metadata": {
        "id": "kRv_fHb08IM3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoB6FESDEJaa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def preprocess_text(text, max_len=256):\n",
        "    ids = encode_text(text, max_len)\n",
        "    pad_len = max_len - len(ids)\n",
        "    if pad_len > 0:\n",
        "        ids = ids + [PAD_IDX] * pad_len  # pad to max_len\n",
        "    else:\n",
        "        ids = ids[:max_len]  # truncate if too long\n",
        "\n",
        "    attention_mask = [1 if id != PAD_IDX else 0 for id in ids]\n",
        "\n",
        "    return torch.tensor([ids]), torch.tensor([attention_mask])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFmggd-3Ncuf"
      },
      "source": [
        "testing the embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmofRg6hmjJ5"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(text, model, device=\"gpu\"):\n",
        "    model.eval()\n",
        "    ids, mask = preprocess_text(text, max_len=256)\n",
        "    ids, mask = ids.to(device), mask.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(ids, mask)\n",
        "        pred = torch.argmax(outputs, dim=1).item()\n",
        "    if pred == 1:\n",
        "      ret = \"world\"\n",
        "    elif pred == 2:\n",
        "      ret = \"Sports\"\n",
        "    elif pred == 3:\n",
        "      ret = \"Business\"\n",
        "    else:\n",
        "      ret = \"sci/Tech\"\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GLo8OW2rxIx",
        "outputId": "03bef863-aac9-4841-c6ef-32c0d0e02656"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "world\n"
          ]
        }
      ],
      "source": [
        "sample_text = \"Aussie equestrian hopes end in sixth Australia #39;s dreams of an historic fourth successive three-day eventing gold medal ended in disappointment but there was still joy for the team when Andrew Hoy #39;s wife won dual gold medals.\"\n",
        "print(predict_sentiment(sample_text, model, device))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zygFSq7PMw8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fivxix1UM3dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nvDmQxH8OPzd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
